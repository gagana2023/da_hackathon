{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40fcd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores parsed to numeric; NaNs: 0\n",
      "Non-integer labels found; value counts of non-integers:\n",
      "score\n",
      "9.5    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Score integer value counts after rounding/clipping:\n",
      "score\n",
      "0       13\n",
      "1        6\n",
      "2        5\n",
      "3        7\n",
      "4        3\n",
      "5        1\n",
      "6       45\n",
      "7       95\n",
      "8      259\n",
      "9     3123\n",
      "10    1443\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "Share of empty fields (train):\n",
      "system_prompt: 0.310\n",
      "prompt: 1.000\n",
      "response: 1.000\n",
      "\n",
      "Share of empty fields (test):\n",
      "system_prompt: 0.304\n",
      "prompt: 1.000\n",
      "response: 1.000\n",
      "\n",
      "Train duplicate combo_hash groups: 594, top group size: 31\n",
      "Train–Test overlap by combo_hash: 551\n",
      "Train–Test overlap by prompt+response hash: 1\n",
      "\n",
      "Top 10 metrics by frequency and score skew:\n",
      "- response_out_of_scope/functional_scope_boundaries | n=56 | mean=9.66\n",
      "score\n",
      "9     19\n",
      "10    37\n",
      "- rejection_rate/under_rejection | n=54 | mean=9.76\n",
      "score\n",
      "9     13\n",
      "10    41\n",
      "- robustness_against_adversarial_attacks/jailbreak_prompts | n=52 | mean=9.56\n",
      "score\n",
      "7      1\n",
      "9     20\n",
      "10    31\n",
      "- misuse/instruction_misuse | n=52 | mean=9.63\n",
      "score\n",
      "9     19\n",
      "10    33\n",
      "- inappropriate_content_detection_rate/sexual_content_detection | n=52 | mean=9.33\n",
      "score\n",
      "9     35\n",
      "10    17\n",
      "- exaggerated_safety/cultural_overcaution | n=51 | mean=9.27\n",
      "score\n",
      "9     37\n",
      "10    14\n",
      "- accuracy | n=51 | mean=9.08\n",
      "score\n",
      "8      3\n",
      "9     41\n",
      "10     7\n",
      "- transliterated_language_handling/tolerance_to_spelling_variants | n=51 | mean=8.63\n",
      "score\n",
      "6      1\n",
      "7      4\n",
      "8     10\n",
      "9     34\n",
      "10     2\n",
      "- data_integrity | n=51 | mean=8.73\n",
      "score\n",
      "0      2\n",
      "8      1\n",
      "9     43\n",
      "10     5\n",
      "- confidence_agreement/overconfident_incorrectness | n=49 | mean=9.45\n",
      "score\n",
      "9     27\n",
      "10    22\n",
      "\n",
      "Prepared grouped CV key: group_key (metric_name##combo_hash).\n",
      "Saved: eda_train_slim.csv, eda_test_slim.csv\n"
     ]
    }
   ],
   "source": [
    "# EDA Cell  Clean schema, fix anomalies, quantify leakage, prep grouped CV keys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json, re, hashlib\n",
    "from collections import Counter\n",
    "\n",
    "# load files\n",
    "def load_jsonl_or_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        txt = f.read().strip()\n",
    "    if not txt:\n",
    "        return []\n",
    "    if txt[0] == \"[\":\n",
    "        return json.loads(txt)\n",
    "    return [json.loads(line) for line in txt.splitlines() if line.strip()]\n",
    "\n",
    "train_path = \"train_data.json\"\n",
    "test_path = \"test_data.json\"\n",
    "metric_names_path = \"metric_names.json\"\n",
    "metric_emb_path = \"metric_name_embeddings.npy\"\n",
    "\n",
    "train = load_jsonl_or_json(train_path)\n",
    "test = load_jsonl_or_json(test_path)\n",
    "\n",
    "df_tr = pd.DataFrame([\n",
    "    {\n",
    "        \"metric_name\": r.get(\"metric_name\", \"\"),\n",
    "        \"prompt\": r.get(\"prompt\", \"\"),\n",
    "        \"system_prompt\": r.get(\"system_prompt\", \"\"),\n",
    "        \"response\": r.get(\"expected_response\", \"\"),\n",
    "        \"score\": r.get(\"score\", None),\n",
    "    } for r in train\n",
    "])\n",
    "\n",
    "df_te = pd.DataFrame([\n",
    "    {\n",
    "        \"metric_name\": r.get(\"metric_name\", \"\"),\n",
    "        \"prompt\": r.get(\"prompt\", \"\"),\n",
    "        \"system_prompt\": r.get(\"system_prompt\", \"\"),\n",
    "        \"response\": r.get(\"expected_response\", \"\"),\n",
    "    } for r in test\n",
    "])\n",
    "\n",
    "# 1) Normalize score column to numeric integer in [0,10]\n",
    "df_tr[\"score_raw\"] = df_tr[\"score\"]\n",
    "df_tr[\"score\"] = pd.to_numeric(df_tr[\"score\"], errors=\"coerce\")\n",
    "n_nan_scores = int(df_tr[\"score\"].isna().sum())\n",
    "print(\"Scores parsed to numeric; NaNs:\", n_nan_scores)\n",
    "\n",
    "if (df_tr[\"score\"] % 1 != 0).any():\n",
    "    print(\"Non-integer labels found; value counts of non-integers:\")\n",
    "    print(df_tr.loc[(df_tr[\"score\"] % 1 != 0), \"score\"].value_counts())\n",
    "    df_tr[\"score\"] = df_tr[\"score\"].round().clip(0, 10)\n",
    "\n",
    "df_tr[\"score\"] = df_tr[\"score\"].astype(\"Int64\")\n",
    "\n",
    "print(\"\\nScore integer value counts after rounding/clipping:\")\n",
    "print(df_tr[\"score\"].value_counts().sort_index())\n",
    "\n",
    "# 2) Fill missing text fields with empty strings; compute lengths\n",
    "for col in [\"system_prompt\", \"prompt\", \"response\"]:\n",
    "    df_tr[col] = df_tr[col].fillna(\"\")\n",
    "    df_te[col] = df_te[col].fillna(\"\")\n",
    "    df_tr[f\"{col}_len\"] = df_tr[col].map(len)\n",
    "    df_te[f\"{col}_len\"] = df_te[col].map(len)\n",
    "\n",
    "print(\"\\nShare of empty fields (train):\")\n",
    "for col in [\"system_prompt\", \"prompt\", \"response\"]:\n",
    "    print(f\"{col}: {float((df_tr[col].str.len()==0).mean()):.3f}\")\n",
    "\n",
    "print(\"\\nShare of empty fields (test):\")\n",
    "for col in [\"system_prompt\", \"prompt\", \"response\"]:\n",
    "    print(f\"{col}: {float((df_te[col].str.len()==0).mean()):.3f}\")\n",
    "\n",
    "# 3) Construct robust combo hashes; add simpler prompt+response hash too\n",
    "def normalize_text(s: str):\n",
    "    s = s or \"\"\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def md5(s: str):\n",
    "    return hashlib.md5(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def combo_hash_row(r):\n",
    "    key = \"|||\".join([\n",
    "        normalize_text(r.get(\"system_prompt\",\"\")),\n",
    "        normalize_text(r.get(\"prompt\",\"\")),\n",
    "        normalize_text(r.get(\"response\",\"\")),\n",
    "        normalize_text(r.get(\"metric_name\",\"\")),\n",
    "    ])\n",
    "    return md5(key)\n",
    "\n",
    "def pr_hash_row(r):\n",
    "    key = \"|||\".join([\n",
    "        normalize_text(r.get(\"prompt\",\"\")),\n",
    "        normalize_text(r.get(\"response\",\"\")),\n",
    "    ])\n",
    "    return md5(key)\n",
    "\n",
    "df_tr[\"combo_hash\"] = df_tr.apply(combo_hash_row, axis=1)\n",
    "df_te[\"combo_hash\"] = df_te.apply(combo_hash_row, axis=1)\n",
    "df_tr[\"pr_hash\"] = df_tr.apply(pr_hash_row, axis=1)\n",
    "df_te[\"pr_hash\"] = df_te.apply(pr_hash_row, axis=1)\n",
    "\n",
    "# 4) Duplicate/leakage quantification\n",
    "dup_counts = df_tr[\"combo_hash\"].value_counts()\n",
    "dup_groups = (dup_counts > 1).sum()\n",
    "max_dup = int(dup_counts.max())\n",
    "print(f\"\\nTrain duplicate combo_hash groups: {dup_groups}, top group size: {max_dup}\")\n",
    "\n",
    "overlap_combo = set(df_tr[\"combo_hash\"]).intersection(set(df_te[\"combo_hash\"]))\n",
    "overlap_pr = set(df_tr[\"pr_hash\"]).intersection(set(df_te[\"pr_hash\"]))\n",
    "print(\"Train–Test overlap by combo_hash:\", len(overlap_combo))\n",
    "print(\"Train–Test overlap by prompt+response hash:\", len(overlap_pr))\n",
    "\n",
    "# 5) Propose leakage-aware CV keys:\n",
    "#    Group by combo_hash for strict de-duplication AND by metric_name cluster key (proxy: metric_name itself).\n",
    "#    We'll create a composite group key to be used with GroupKFold later.\n",
    "df_tr[\"group_key\"] = df_tr[\"metric_name\"].astype(str) + \"##\" + df_tr[\"combo_hash\"]\n",
    "\n",
    "# 6) Quick per-metric score distribution summary for the top metrics\n",
    "metric_freq = df_tr[\"metric_name\"].value_counts()\n",
    "top_metrics = metric_freq.head(10).index.tolist()\n",
    "print(\"\\nTop 10 metrics by frequency and score skew:\")\n",
    "for m in top_metrics:\n",
    "    vc = df_tr.loc[df_tr[\"metric_name\"] == m, \"score\"].value_counts().sort_index()\n",
    "    print(f\"- {m} | n={int(metric_freq[m])} | mean={float(df_tr.loc[df_tr['metric_name']==m,'score'].mean()):.2f}\")\n",
    "    print(vc.to_string())\n",
    "\n",
    "# 7) Save slim EDA cache safely \n",
    "for name, df, has_score in [\n",
    "    (\"eda_train_slim.csv\", df_tr, True),\n",
    "    (\"eda_test_slim.csv\", df_te, False),\n",
    "]:\n",
    "    cols = [\"metric_name\", \"system_prompt_len\", \"prompt_len\", \"response_len\", \"combo_hash\", \"pr_hash\"]\n",
    "    if has_score and \"score\" in df.columns:\n",
    "        cols = [\"metric_name\", \"system_prompt_len\", \"prompt_len\", \"response_len\", \"score\", \"combo_hash\", \"pr_hash\"]\n",
    "    df[cols].to_csv(name, index=False)\n",
    "\n",
    "print(\"\\nPrepared grouped CV key: group_key (metric_name##combo_hash).\")\n",
    "print(\"Saved: eda_train_slim.csv, eda_test_slim.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8071048f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (5000, 810)  Target shape: (5000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: MAE=2.3960  (n_va=1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2: MAE=2.3950  (n_va=1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3: MAE=2.2310  (n_va=1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4: MAE=2.0960  (n_va=1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5: MAE=2.1940  (n_va=1000)\n",
      "\n",
      "OOF MAE (metric-only, leakage-safe groups): 2.2624\n",
      "OOF QWK: 0.0556\n",
      "Saved oof_metric_only.csv\n"
     ]
    }
   ],
   "source": [
    "# EDA Cell 3: Leakage-safe CV + metric-only baseline \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, re, hashlib\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load slim caches from previous cell\n",
    "df_tr = pd.read_csv(\"eda_train_slim.csv\")\n",
    "df_te = pd.read_csv(\"eda_test_slim.csv\")\n",
    "metric_names = json.load(open(\"metric_names.json\",\"r\", encoding=\"utf-8\"))\n",
    "metric_emb = np.load(\"metric_name_embeddings.npy\")\n",
    "\n",
    "# Map metric_name -> embedding row\n",
    "name_to_idx = {name: i for i, name in enumerate(metric_names)}\n",
    "def get_metric_vec(name):\n",
    "    idx = name_to_idx.get(name, None)\n",
    "    if idx is None:\n",
    "        return np.zeros(metric_emb.shape[1], dtype=np.float32)\n",
    "    return metric_emb[idx]\n",
    "\n",
    "# Reattach columns needed for grouping and targets by reloading full train\n",
    "def load_jsonl_or_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        txt = f.read().strip()\n",
    "    if not txt:\n",
    "        return []\n",
    "    if txt[0] == \"[\":\n",
    "        return json.loads(txt)\n",
    "    return [json.loads(line) for line in txt.splitlines() if line.strip()]\n",
    "\n",
    "train = load_jsonl_or_json(\"train_data.json\")\n",
    "full_tr = pd.DataFrame([{\n",
    "    \"metric_name\": r.get(\"metric_name\",\"\"),\n",
    "    \"prompt\": r.get(\"prompt\",\"\"),\n",
    "    \"system_prompt\": r.get(\"system_prompt\",\"\") or \"\",\n",
    "    \"response\": r.get(\"expected_response\",\"\"),\n",
    "    \"score\": r.get(\"score\", None),\n",
    "} for r in train])\n",
    "\n",
    "# Normalize scores \n",
    "full_tr[\"score\"] = pd.to_numeric(full_tr[\"score\"], errors=\"coerce\").round().clip(0,10).astype(\"Int64\")\n",
    "\n",
    "# Recompute combo_hash for grouping \n",
    "import re, hashlib\n",
    "def normalize_text(s):\n",
    "    s = s or \"\"\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def md5(s):\n",
    "    return hashlib.md5(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def combo_hash_row(r):\n",
    "    key = \"|||\".join([\n",
    "        normalize_text(r.get(\"system_prompt\",\"\")),\n",
    "        normalize_text(r.get(\"prompt\",\"\")),\n",
    "        normalize_text(r.get(\"response\",\"\")),\n",
    "        normalize_text(r.get(\"metric_name\",\"\")),\n",
    "    ])\n",
    "    return md5(key)\n",
    "\n",
    "full_tr[\"combo_hash\"] = full_tr.apply(combo_hash_row, axis=1)\n",
    "\n",
    "# Build metric-only features:\n",
    "# - 768-d Gemma embedding\n",
    "# - One-hot of top-N metric names (N=40) to capture head metrics\n",
    "# - Basic length of system_prompt (since prompt/response are empty)\n",
    "N_TOP = 40\n",
    "top_metrics = full_tr[\"metric_name\"].value_counts().head(N_TOP).index.tolist()\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "ohe.fit(pd.DataFrame({\"metric_name\": top_metrics + [\"__OTHER__\"]}))\n",
    "\n",
    "def row_features(row):\n",
    "    name = row[\"metric_name\"]\n",
    "    vec = get_metric_vec(name)\n",
    "    # one-hot\n",
    "    ohe_input = pd.DataFrame({\"metric_name\": [name if name in top_metrics else \"__OTHER__\"]})\n",
    "    oh = ohe.transform(ohe_input)[0]\n",
    "    sys_len = len(row[\"system_prompt\"] or \"\")\n",
    "    return np.concatenate([vec, oh, np.array([sys_len], dtype=np.float32)], axis=0)\n",
    "\n",
    "X = np.vstack([row_features(r) for _, r in full_tr.iterrows()])\n",
    "y = full_tr[\"score\"].astype(int).values\n",
    "groups = full_tr[\"combo_hash\"].values  # leakage-safe grouping\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape, \" Target shape:\", y.shape)\n",
    "\n",
    ".\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "classes = np.unique(y)\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y)\n",
    "cw_map = {c: w for c, w in zip(classes, class_weights)}\n",
    "sample_weight = np.array([cw_map[v] for v in y], dtype=np.float32)\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "oof_pred = np.zeros((len(y), len(classes)), dtype=np.float32)\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(gkf.split(X, y, groups=groups), 1):\n",
    "    X_tr, y_tr, w_tr = X[tr_idx], y[tr_idx], sample_weight[tr_idx]\n",
    "    X_va, y_va = X[va_idx], y[va_idx]\n",
    "\n",
    "    clf = LogisticRegression(\n",
    "        multi_class=\"multinomial\",\n",
    "        max_iter=2000,\n",
    "        C=1.0,\n",
    "        n_jobs=4,\n",
    "        class_weight=None,  # using sample_weight instead\n",
    "        solver=\"lbfgs\",\n",
    "    )\n",
    "    clf.fit(X_tr, y_tr, sample_weight=w_tr)\n",
    "    proba = clf.predict_proba(X_va)\n",
    "    # Align columns to classes\n",
    "    # sklearn orders by classes_ attribute\n",
    "    col_order = list(clf.classes_)\n",
    "    col_map = {c:i for i,c in enumerate(col_order)}\n",
    "    # Fill into oof_pred at indices corresponding to va_idx and proper columns\n",
    "    for i_c, c in enumerate(classes):\n",
    "        if c in col_map:\n",
    "            oof_pred[va_idx, i_c] = proba[:, col_map[c]]\n",
    "        else:\n",
    "            # unseen class in this fold\n",
    "            oof_pred[va_idx, i_c] = 0.0\n",
    "\n",
    "    y_pred_int = classes[np.argmax(oof_pred[va_idx], axis=1)]\n",
    "    mae = mean_absolute_error(y_va, y_pred_int)\n",
    "    print(f\"Fold {fold}: MAE={mae:.4f}  (n_va={len(va_idx)})\")\n",
    "\n",
    "# Report overall OOF\n",
    "y_pred_int_all = classes[np.argmax(oof_pred, axis=1)]\n",
    "mae_all = mean_absolute_error(y, y_pred_int_all)\n",
    "print(f\"\\nOOF MAE (metric-only, leakage-safe groups): {mae_all:.4f}\")\n",
    "\n",
    "#  quadratic weighted kappa\n",
    "def quadratic_weighted_kappa(a, b, min_rating=None, max_rating=None):\n",
    "    # a,b are integer arrays\n",
    "    if min_rating is None: min_rating = int(min(a.min(), b.min()))\n",
    "    if max_rating is None: max_rating = int(max(a.max(), b.max()))\n",
    "    n_ratings = max_rating - min_rating + 1\n",
    "    conf_mat = np.zeros((n_ratings, n_ratings), dtype=np.float64)\n",
    "    for i in range(len(a)):\n",
    "        conf_mat[a[i]-min_rating, b[i]-min_rating] += 1\n",
    "    hist_a = conf_mat.sum(axis=1)\n",
    "    hist_b = conf_mat.sum(axis=0)\n",
    "    expected = np.outer(hist_a, hist_b) / conf_mat.sum()\n",
    "    w = np.zeros((n_ratings, n_ratings), dtype=np.float64)\n",
    "    for i in range(n_ratings):\n",
    "        for j in range(n_ratings):\n",
    "            w[i,j] = ((i-j)**2) / ((n_ratings-1)**2)\n",
    "    kappa = 1.0 - (w * conf_mat).sum() / (w * expected).sum()\n",
    "    return float(kappa)\n",
    "\n",
    "qwk_all = quadratic_weighted_kappa(y, y_pred_int_all, min_rating=0, max_rating=10)\n",
    "print(f\"OOF QWK: {qwk_all:.4f}\")\n",
    "\n",
    "# Persist OOF for later calibration\n",
    "oof_df = pd.DataFrame({\n",
    "    \"score_true\": y,\n",
    "    \"score_pred\": y_pred_int_all\n",
    "})\n",
    "oof_df.to_csv(\"oof_metric_only.csv\", index=False)\n",
    "print(\"Saved oof_metric_only.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbf56c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: train=4000 valid=1000\n",
      "  Fold 1 MAE: 2.0640\n",
      "Fold 2: train=4000 valid=1000\n",
      "  Fold 2 MAE: 2.1110\n",
      "Fold 3: train=4000 valid=1000\n",
      "  Fold 3 MAE: 3.0330\n",
      "Fold 4: train=4000 valid=1000\n",
      "  Fold 4 MAE: 1.2080\n",
      "Fold 5: train=4000 valid=1000\n",
      "  Fold 5 MAE: 2.1800\n",
      "\n",
      "Ordinal OOF MAE: 2.1192\n",
      "Ordinal OOF QWK: -0.0739\n",
      "Saved oof_ordinal_metric_only.csv\n"
     ]
    }
   ],
   "source": [
    "# Model Cell: Ordinal cumulative baseline with fold-safe weights + metric priors (leakage-safe OOF)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, re, hashlib, warnings\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# -------------------\n",
    "# Load data\n",
    "# -------------------\n",
    "def load_jsonl_or_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        txt = f.read().strip()\n",
    "    if not txt:\n",
    "        return []\n",
    "    if txt[0] == \"[\":\n",
    "        return json.loads(txt)\n",
    "    return [json.loads(line) for line in txt.splitlines() if line.strip()]\n",
    "\n",
    "train = load_jsonl_or_json(\"train_data.json\")\n",
    "metric_names = json.load(open(\"metric_names.json\",\"r\", encoding=\"utf-8\"))\n",
    "metric_emb = np.load(\"metric_name_embeddings.npy\")\n",
    "\n",
    "name_to_idx = {name: i for i, name in enumerate(metric_names)}\n",
    "\n",
    "df = pd.DataFrame([{\n",
    "    \"metric_name\": r.get(\"metric_name\",\"\"),\n",
    "    \"prompt\": r.get(\"prompt\",\"\"),\n",
    "    \"system_prompt\": (r.get(\"system_prompt\",\"\") or \"\"),\n",
    "    \"response\": r.get(\"expected_response\",\"\"),\n",
    "    \"score\": r.get(\"score\", None),\n",
    "} for r in train])\n",
    "\n",
    "# Normalize scores to integer 0..10 (handles stray 9.5)\n",
    "df[\"score\"] = pd.to_numeric(df[\"score\"], errors=\"coerce\").round().clip(0,10).astype(int)\n",
    "y_all = df[\"score\"].values\n",
    "K = 11\n",
    "classes = np.arange(K)\n",
    "\n",
    "# -------------------\n",
    "# Combo-hash groups (leakage-safe)\n",
    "# -------------------\n",
    "def normalize_text(s):\n",
    "    s = s or \"\"\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def md5(s):\n",
    "    return hashlib.md5(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def combo_hash_row(r):\n",
    "    key = \"|||\".join([\n",
    "        normalize_text(r.get(\"system_prompt\",\"\")),\n",
    "        normalize_text(r.get(\"prompt\",\"\")),\n",
    "        normalize_text(r.get(\"response\",\"\")),\n",
    "        normalize_text(r.get(\"metric_name\",\"\")),\n",
    "    ])\n",
    "    return md5(key)\n",
    "\n",
    "df[\"combo_hash\"] = df.apply(combo_hash_row, axis=1)\n",
    "groups = df[\"combo_hash\"].values\n",
    "\n",
    "# -------------------\n",
    "# Features: Gemma vector + per-metric prior (fold-train only) + system_prompt length\n",
    "# -------------------\n",
    "def get_metric_vec(name):\n",
    "    idx = name_to_idx.get(name, None)\n",
    "    if idx is None:\n",
    "        return np.zeros(metric_emb.shape[1], dtype=np.float32)\n",
    "    return metric_emb[idx]\n",
    "\n",
    "def build_fold_features(idx_train):\n",
    "    df_tr = df.iloc[idx_train]\n",
    "    priors = df_tr.groupby(\"metric_name\")[\"score\"].mean().to_dict()\n",
    "    gmean = float(df_tr[\"score\"].mean())\n",
    "    feats = []\n",
    "    for _, r in df.iterrows():\n",
    "        name = r[\"metric_name\"]\n",
    "        vec = get_metric_vec(name)\n",
    "        prior = priors.get(name, gmean)\n",
    "        sys_len = len(r[\"system_prompt\"])\n",
    "        feats.append(np.concatenate([vec, np.array([prior, sys_len], dtype=np.float32)], axis=0))\n",
    "    X_all = np.vstack(feats).astype(np.float32)\n",
    "    return X_all\n",
    "\n",
    "# -------------------\n",
    "# Ordinal cumulative targets and utilities\n",
    "# -------------------\n",
    "def cumulative_targets(y, K=11):\n",
    "    # For thresholds t=0..K-2, target is 1 if y <= t else 0\n",
    "    T = np.zeros((len(y), K-1), dtype=np.float32)\n",
    "    for i, yi in enumerate(y):\n",
    "        # 0..yi-1 -> 0, yi..K-2 -> 1\n",
    "        if yi < (K-1):\n",
    "            T[i, :yi] = 0.0\n",
    "            T[i, yi:] = 1.0\n",
    "        else:\n",
    "            # yi == 10, all thresholds predict 1\n",
    "            T[i, :] = 1.0\n",
    "    return T\n",
    "\n",
    "def probs_from_cum(sig):\n",
    "    # sig shape [B, K-1] in (0,1); return class probs [B, K]\n",
    "    B, Km1 = sig.shape\n",
    "    P_le = torch.cat([torch.zeros(B,1,device=sig.device), sig], dim=1)\n",
    "    P_gt = torch.cat([sig, torch.ones(B,1,device=sig.device)], dim=1)\n",
    "    return (P_gt - P_le).clamp(1e-7, 1.0)\n",
    "\n",
    "def quadratic_weighted_kappa(a, b, min_rating=0, max_rating=10):\n",
    "    n_ratings = max_rating - min_rating + 1\n",
    "    conf_mat = np.zeros((n_ratings, n_ratings), dtype=np.float64)\n",
    "    for i in range(len(a)):\n",
    "        conf_mat[a[i]-min_rating, b[i]-min_rating] += 1\n",
    "    hist_a = conf_mat.sum(axis=1)\n",
    "    hist_b = conf_mat.sum(axis=0)\n",
    "    expected = np.outer(hist_a, hist_b) / conf_mat.sum()\n",
    "    w = np.zeros((n_ratings, n_ratings), dtype=np.float64)\n",
    "    for i in range(n_ratings):\n",
    "        for j in range(n_ratings):\n",
    "            w[i,j] = ((i-j)**2) / ((n_ratings-1)**2)\n",
    "    kappa = 1.0 - (w * conf_mat).sum() / (w * expected).sum()\n",
    "    return float(kappa)\n",
    "\n",
    "# -------------------\n",
    "# Model\n",
    "# -------------------\n",
    "class OrdinalCumulativeNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=256, K=11):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        self.head = nn.Linear(hidden, K-1)  # thresholds\n",
    "    def forward(self, x):\n",
    "        h = self.backbone(x)\n",
    "        logits = self.head(h)\n",
    "        # enforce monotonicity by cumulative sum across thresholds\n",
    "        logits = torch.cumsum(logits, dim=1)\n",
    "        return logits\n",
    "\n",
    "# -------------------\n",
    "# Training with fold-safe threshold weights\n",
    "# -------------------\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "oof_proba = np.zeros((len(df), K), dtype=np.float32)\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(gkf.split(df, y_all, groups), 1):\n",
    "    print(f\"Fold {fold}: train={len(tr_idx)} valid={len(va_idx)}\")\n",
    "    X_all = build_fold_features(tr_idx)\n",
    "    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n",
    "    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n",
    "\n",
    "    # Fold-safe inverse-frequency weights on present classes\n",
    "    labels_present, counts = np.unique(y_tr, return_counts=True)\n",
    "    freq = counts / counts.sum()\n",
    "    inv = {int(lbl): float(1.0 / f) for lbl, f in zip(labels_present, freq)}\n",
    "    mean_inv = np.mean(list(inv.values()))\n",
    "    inv = {k: v / (mean_inv + 1e-12) for k, v in inv.items()}\n",
    "\n",
    "    # Build threshold weights from class weights (emphasize rare lower labels)\n",
    "    th_w = np.zeros(K-1, dtype=np.float32)\n",
    "    for t in range(K-1):\n",
    "        w_left = inv.get(t, 1.0)\n",
    "        w_right = inv.get(t+1, 1.0)\n",
    "        th_w[t] = 0.5 * (w_left + w_right)\n",
    "    th_w = torch.tensor(th_w, dtype=torch.float32)\n",
    "\n",
    "    # Targets with light label smoothing for stability\n",
    "    eps = 0.02\n",
    "    Yc = cumulative_targets(y_tr, K)\n",
    "    Yc = Yc * (1 - eps) + 0.5 * eps\n",
    "    y_tr_T = torch.tensor(Yc, dtype=torch.float32)\n",
    "\n",
    "    # Tensors\n",
    "    X_tr_t = torch.tensor(X_tr, dtype=torch.float32)\n",
    "    X_va_t = torch.tensor(X_va, dtype=torch.float32)\n",
    "\n",
    "    model = OrdinalCumulativeNN(in_dim=X_tr.shape[1], hidden=256, K=K)\n",
    "    opt = optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-4)\n",
    "    bce = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "    # Train\n",
    "    EPOCHS = 80\n",
    "    B = 512\n",
    "    n = len(X_tr_t)\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        perm = torch.randperm(n)\n",
    "        for i in range(0, n, B):\n",
    "            idx = perm[i:i+B]\n",
    "            xb = X_tr_t[idx]\n",
    "            yb = y_tr_T[idx]\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss_raw = bce(logits, yb)  # [B, K-1]\n",
    "            loss = (loss_raw * th_w).mean()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sig = torch.sigmoid(model(X_va_t))\n",
    "        P = probs_from_cum(sig)\n",
    "        oof_proba[va_idx] = P.cpu().numpy()\n",
    "\n",
    "    y_pred = oof_proba[va_idx].argmax(axis=1)\n",
    "    mae = np.mean(np.abs(y_va - y_pred))\n",
    "    print(f\"  Fold {fold} MAE: {mae:.4f}\")\n",
    "\n",
    "# -------------------\n",
    "# OOF metrics\n",
    "# -------------------\n",
    "y_pred_all = oof_proba.argmax(axis=1)\n",
    "mae_all = np.mean(np.abs(y_all - y_pred_all))\n",
    "qwk_all = quadratic_weighted_kappa(y_all, y_pred_all, 0, 10)\n",
    "\n",
    "print(f\"\\nOrdinal OOF MAE: {mae_all:.4f}\")\n",
    "print(f\"Ordinal OOF QWK: {qwk_all:.4f}\")\n",
    "\n",
    "# Save for calibration next\n",
    "cols = {f\"p{k}\": oof_proba[:,k] for k in range(K)}\n",
    "oof_ord = pd.DataFrame({\"score_true\": y_all, **cols})\n",
    "oof_ord.to_csv(\"oof_ordinal_metric_only.csv\", index=False)\n",
    "print(\"Saved oof_ordinal_metric_only.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90164591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: train=4000 valid=1000\n",
      "  Fold 1 MAE (blended): 0.7250\n",
      "Fold 2: train=4000 valid=1000\n",
      "  Fold 2 MAE (blended): 1.1140\n",
      "Fold 3: train=4000 valid=1000\n",
      "  Fold 3 MAE (blended): 1.6910\n",
      "Fold 4: train=4000 valid=1000\n",
      "  Fold 4 MAE (blended): 1.1180\n",
      "Fold 5: train=4000 valid=1000\n",
      "  Fold 5 MAE (blended): 1.3990\n",
      "\n",
      "Ordinal+Prior OOF MAE: 1.2094\n",
      "Ordinal+Prior OOF QWK: -0.0864\n",
      "Prior-only OOF MAE (floor): 0.5106\n",
      "Prior-only OOF QWK (floor): 0.1090\n",
      "Saved oof_ordinal_prior_blend.csv\n"
     ]
    }
   ],
   "source": [
    "# Model Cell: Smoothed per-metric priors + stabilized ordinal training and prior baseline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, re, hashlib, warnings\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.manual_seed(123)\n",
    "np.random.seed(123)\n",
    "\n",
    "# Load\n",
    "def load_jsonl_or_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        txt = f.read().strip()\n",
    "    if not txt:\n",
    "        return []\n",
    "    if txt[0] == \"[\":\n",
    "        return json.loads(txt)\n",
    "    return [json.loads(line) for line in txt.splitlines() if line.strip()]\n",
    "\n",
    "train = load_jsonl_or_json(\"train_data.json\")\n",
    "metric_names = json.load(open(\"metric_names.json\",\"r\", encoding=\"utf-8\"))\n",
    "metric_emb = np.load(\"metric_name_embeddings.npy\")\n",
    "name_to_idx = {name: i for i, name in enumerate(metric_names)}\n",
    "\n",
    "df = pd.DataFrame([{\n",
    "    \"metric_name\": r.get(\"metric_name\",\"\"),\n",
    "    \"prompt\": r.get(\"prompt\",\"\"),\n",
    "    \"system_prompt\": (r.get(\"system_prompt\",\"\") or \"\"),\n",
    "    \"response\": r.get(\"expected_response\",\"\"),\n",
    "    \"score\": r.get(\"score\", None),\n",
    "} for r in train])\n",
    "df[\"score\"] = pd.to_numeric(df[\"score\"], errors=\"coerce\").round().clip(0,10).astype(int)\n",
    "y_all = df[\"score\"].values\n",
    "K = 11\n",
    "\n",
    "# Group key\n",
    "def normalize_text(s):\n",
    "    s = s or \"\"\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "def md5(s):\n",
    "    return hashlib.md5(s.encode(\"utf-8\")).hexdigest()\n",
    "def combo_hash_row(r):\n",
    "    key = \"|||\".join([\n",
    "        normalize_text(r.get(\"system_prompt\",\"\")),\n",
    "        normalize_text(r.get(\"prompt\",\"\")),\n",
    "        normalize_text(r.get(\"response\",\"\")),\n",
    "        normalize_text(r.get(\"metric_name\",\"\")),\n",
    "    ])\n",
    "    return md5(key)\n",
    "df[\"combo_hash\"] = df.apply(combo_hash_row, axis=1)\n",
    "groups = df[\"combo_hash\"].values\n",
    "\n",
    "def get_metric_vec(name):\n",
    "    idx = name_to_idx.get(name, None)\n",
    "    if idx is None:\n",
    "        return np.zeros(metric_emb.shape[1], dtype=np.float32)\n",
    "    return metric_emb[idx]\n",
    "\n",
    "# Compute fold-safe priors p(y|metric) with Laplace smoothing\n",
    "def compute_priors(df_fold):\n",
    "    counts = df_fold.groupby([\"metric_name\",\"score\"]).size().unstack(fill_value=0)\n",
    "    # ensure all classes present\n",
    "    for c in range(K):\n",
    "        if c not in counts.columns:\n",
    "            counts[c] = 0\n",
    "    counts = counts[sorted(counts.columns)]\n",
    "    # Laplace smoothing\n",
    "    alpha = 1.0\n",
    "    counts_sm = counts + alpha\n",
    "    probs = counts_sm.div(counts_sm.sum(axis=1), axis=0)\n",
    "    means = (probs * np.arange(K)).sum(axis=1)\n",
    "    return probs, means  # DataFrame (metric x K), Series (metric)\n",
    "\n",
    "# Ordinal model\n",
    "class OrdinalCumulativeNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=128, K=11, l2_thresh=1e-3):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "        self.head = nn.Linear(hidden, K-1)\n",
    "        self.l2_thresh = l2_thresh\n",
    "    def forward(self, x):\n",
    "        h = self.backbone(x)\n",
    "        logits = self.head(h)\n",
    "        logits = torch.cumsum(logits, dim=1)\n",
    "        return logits\n",
    "    def reg(self):\n",
    "        return sum((p**2).sum() for p in self.head.parameters()) * self.l2_thresh\n",
    "\n",
    "def cumulative_targets(y, K=11):\n",
    "    T = np.zeros((len(y), K-1), dtype=np.float32)\n",
    "    for i, yi in enumerate(y):\n",
    "        if yi < (K-1):\n",
    "            T[i, :yi] = 0.0\n",
    "            T[i, yi:] = 1.0\n",
    "        else:\n",
    "            T[i, :] = 1.0\n",
    "    return T\n",
    "\n",
    "def probs_from_cum(sig):\n",
    "    B, Km1 = sig.shape\n",
    "    P_le = torch.cat([torch.zeros(B,1,device=sig.device), sig], dim=1)\n",
    "    P_gt = torch.cat([sig, torch.ones(B,1,device=sig.device)], dim=1)\n",
    "    return (P_gt - P_le).clamp(1e-7, 1.0)\n",
    "\n",
    "def quadratic_weighted_kappa(a, b, min_rating=0, max_rating=10):\n",
    "    n_ratings = max_rating - min_rating + 1\n",
    "    conf_mat = np.zeros((n_ratings, n_ratings), dtype=np.float64)\n",
    "    for i in range(len(a)):\n",
    "        conf_mat[a[i]-min_rating, b[i]-min_rating] += 1\n",
    "    hist_a = conf_mat.sum(axis=1)\n",
    "    hist_b = conf_mat.sum(axis=0)\n",
    "    expected = np.outer(hist_a, hist_b) / conf_mat.sum()\n",
    "    w = np.zeros((n_ratings, n_ratings), dtype=np.float64)\n",
    "    for i in range(n_ratings):\n",
    "        for j in range(n_ratings):\n",
    "            w[i,j] = ((i-j)**2) / ((n_ratings-1)**2)\n",
    "    kappa = 1.0 - (w * conf_mat).sum() / (w * expected).sum()\n",
    "    return float(kappa)\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "oof_proba = np.zeros((len(df), K), dtype=np.float32)\n",
    "prior_blend_oof = np.zeros((len(df), K), dtype=np.float32)\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(gkf.split(df, y_all, groups), 1):\n",
    "    print(f\"Fold {fold}: train={len(tr_idx)} valid={len(va_idx)}\")\n",
    "    df_tr = df.iloc[tr_idx].copy()\n",
    "    df_va = df.iloc[va_idx].copy()\n",
    "\n",
    "    probs_m, means_m = compute_priors(df_tr)\n",
    "    global_prior = np.ones(K, dtype=np.float32) / K\n",
    "\n",
    "    # Build features for all rows with fold-train priors (no leakage)\n",
    "    feats = []\n",
    "    priors_for_all = []\n",
    "    for _, r in df.iterrows():\n",
    "        name = r[\"metric_name\"]\n",
    "        vec = get_metric_vec(name)\n",
    "        if name in probs_m.index:\n",
    "            pvec = probs_m.loc[name].values.astype(np.float32)\n",
    "            pmean = float(means_m.loc[name])\n",
    "        else:\n",
    "            pvec = global_prior.copy()\n",
    "            pmean = float((np.arange(K) * global_prior).sum())\n",
    "        sys_len = len(r[\"system_prompt\"])\n",
    "        feats.append(np.concatenate([vec, np.array([pmean, sys_len], dtype=np.float32), pvec], axis=0))\n",
    "        priors_for_all.append(pvec)\n",
    "    X_all = np.vstack(feats).astype(np.float32)\n",
    "    priors_for_all = np.vstack(priors_for_all).astype(np.float32)\n",
    "\n",
    "    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n",
    "    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n",
    "    pri_va = priors_for_all[va_idx]\n",
    "\n",
    "    # Prior-only baseline on validation (as a floor)\n",
    "    prior_pred = pri_va  # already smoothed\n",
    "    prior_blend_oof[va_idx] = prior_pred\n",
    "\n",
    "    # Ordinal training\n",
    "    model = OrdinalCumulativeNN(in_dim=X_tr.shape[1], hidden=128, K=K, l2_thresh=1e-4)\n",
    "    opt = optim.AdamW(model.parameters(), lr=1.5e-3, weight_decay=5e-4)\n",
    "    bce = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "\n",
    "    # Label smoothing\n",
    "    eps = 0.02\n",
    "    Yc = cumulative_targets(y_tr, K)\n",
    "    Yc = Yc * (1 - eps) + 0.5 * eps\n",
    "    y_tr_T = torch.tensor(Yc, dtype=torch.float32)\n",
    "\n",
    "    X_tr_t = torch.tensor(X_tr, dtype=torch.float32)\n",
    "    X_va_t = torch.tensor(X_va, dtype=torch.float32)\n",
    "\n",
    "    EPOCHS = 40\n",
    "    B = 512\n",
    "    n = len(X_tr_t)\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        perm = torch.randperm(n)\n",
    "        for i in range(0, n, B):\n",
    "            idx = perm[i:i+B]\n",
    "            xb = X_tr_t[idx]\n",
    "            yb = y_tr_T[idx]\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = bce(logits, yb) + model.reg()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "    # Inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sig = torch.sigmoid(model(X_va_t))\n",
    "        P = probs_from_cum(sig).cpu().numpy()\n",
    "\n",
    "    #  convex blend with prior to stabilize\n",
    "    alpha = 0.3\n",
    "    P_blend = (1 - alpha) * P + alpha * pri_va\n",
    "    oof_proba[va_idx] = P_blend\n",
    "\n",
    "    y_pred = oof_proba[va_idx].argmax(axis=1)\n",
    "    mae = np.mean(np.abs(y_va - y_pred))\n",
    "    print(f\"  Fold {fold} MAE (blended): {mae:.4f}\")\n",
    "\n",
    "# OOF metrics\n",
    "y_pred_all = oof_proba.argmax(axis=1)\n",
    "mae_all = np.mean(np.abs(y_all - y_pred_all))\n",
    "\n",
    "def qwk(a,b):\n",
    "    return quadratic_weighted_kappa(a,b,0,10)\n",
    "\n",
    "qwk_all = qwk(y_all, y_pred_all)\n",
    "print(f\"\\nOrdinal+Prior OOF MAE: {mae_all:.4f}\")\n",
    "print(f\"Ordinal+Prior OOF QWK: {qwk_all:.4f}\")\n",
    "\n",
    "# Prior-only floor\n",
    "y_prior = prior_blend_oof.argmax(axis=1)\n",
    "print(f\"Prior-only OOF MAE (floor): {np.mean(np.abs(y_all - y_prior)):.4f}\")\n",
    "print(f\"Prior-only OOF QWK (floor): {qwk(y_all, y_prior):.4f}\")\n",
    "\n",
    "# Save\n",
    "cols = {f\"p{k}\": oof_proba[:,k] for k in range(K)}\n",
    "pd.DataFrame({\"score_true\": y_all, **cols}).to_csv(\"oof_ordinal_prior_blend.csv\", index=False)\n",
    "print(\"Saved oof_ordinal_prior_blend.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5addb13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote submission_prior_calibrated_with_cache.csv\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167ebb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen temperatures: (1.0, 1.0, 1.2) objective: -0.36608\n",
      "Pred distribution: {0: 2, 1: 7, 6: 1482, 7: 70, 8: 76, 9: 1336, 10: 665}\n",
      "Wrote submission_calibrated_shrunk_residual_safe.csv\n"
     ]
    }
   ],
   "source": [
    "# Patched calibration + inference with robust NaN handling and safe fallbacks\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, re, hashlib\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "K = 11\n",
    "idxs = np.arange(K)\n",
    "\n",
    "def normalize_text(s):\n",
    "    s = s or \"\"\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def md5(s):\n",
    "    return hashlib.md5(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def combo_hash_row(r):\n",
    "    key = \"|||\".join([\n",
    "        normalize_text(r.get(\"system_prompt\",\"\")),\n",
    "        normalize_text(r.get(\"prompt\",\"\")),\n",
    "        normalize_text(r.get(\"expected_response\",\"\")),\n",
    "        normalize_text(r.get(\"metric_name\",\"\")),\n",
    "    ])\n",
    "    return md5(key)\n",
    "\n",
    "def qwk(a,b, min_rating=0, max_rating=10):\n",
    "    n_ratings = max_rating - min_rating + 1\n",
    "    conf = np.zeros((n_ratings,n_ratings))\n",
    "    for i in range(len(a)):\n",
    "        conf[a[i]-min_rating, b[i]-min_rating] += 1\n",
    "    hist_a = conf.sum(axis=1); hist_b = conf.sum(axis=0)\n",
    "    expected = np.outer(hist_a, hist_b) / conf.sum()\n",
    "    w = np.zeros_like(conf)\n",
    "    for i in range(n_ratings):\n",
    "        for j in range(n_ratings):\n",
    "            w[i,j] = ((i-j)**2)/((n_ratings-1)**2)\n",
    "    return 1.0 - (w*conf).sum() / (w*expected).sum()\n",
    "\n",
    "def safe_softmax(logits):\n",
    "    logits = np.nan_to_num(logits, nan=-1e9, posinf=1e6, neginf=-1e6)\n",
    "    logits = logits - logits.max(axis=1, keepdims=True)\n",
    "    P = np.exp(logits)\n",
    "    P_sum = P.sum(axis=1, keepdims=True)\n",
    "    P = P / np.clip(P_sum, 1e-12, None)\n",
    "    return P\n",
    "\n",
    "def apply_temperatures(P, tau_lo=1.0, tau_mid=1.0, tau_hi=1.0):\n",
    "    P = np.nan_to_num(P, nan=1.0/K)\n",
    "    P = P / np.clip(P.sum(axis=1, keepdims=True), 1e-12, None)\n",
    "    logits = np.log(np.clip(P, 1e-12, 1.0))\n",
    "    scale = np.ones(K, dtype=np.float32)\n",
    "    scale[idxs <= 5] = 1.0 / tau_lo\n",
    "    scale[(idxs >= 6) & (idxs <= 8)] = 1.0 / tau_mid\n",
    "    scale[idxs >= 9] = 1.0 / tau_hi\n",
    "    L = logits * scale[None, :]\n",
    "    return safe_softmax(L)\n",
    "\n",
    "def expected_value(P):\n",
    "    return (P * idxs[None,:]).sum(axis=1)\n",
    "\n",
    "def entropy(P):\n",
    "    P = np.nan_to_num(P, nan=1.0/K)\n",
    "    P = P / np.clip(P.sum(axis=1, keepdims=True), 1e-12, None)\n",
    "    return -(P * np.log(np.clip(P, 1e-12, 1.0))).sum(axis=1)\n",
    "\n",
    "# Load\n",
    "train = json.load(open(\"train_data.json\",\"r\",encoding=\"utf-8\"))\n",
    "test = json.load(open(\"test_data.json\",\"r\",encoding=\"utf-8\"))\n",
    "df_tr = pd.DataFrame(train)\n",
    "df_tr[\"score\"] = pd.to_numeric(df_tr[\"score\"], errors=\"coerce\").round().clip(0,10).astype(int)\n",
    "df_tr[\"combo_hash\"] = df_tr.apply(combo_hash_row, axis=1)\n",
    "df_te = pd.DataFrame(test)\n",
    "df_te[\"combo_hash\"] = df_te.apply(combo_hash_row, axis=1)\n",
    "\n",
    "dup_cache = dict(zip(df_tr[\"combo_hash\"], df_tr[\"score\"]))\n",
    "y_true = df_tr[\"score\"].values\n",
    "groups = df_tr[\"combo_hash\"].values\n",
    "\n",
    "\n",
    "alpha = 1.0\n",
    "lambda_shrink = 20  \n",
    "\n",
    "def build_oof_priors(lambda_shrink):\n",
    "    oof_P = np.zeros((len(df_tr), K), dtype=np.float32)\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    for tr_idx, va_idx in gkf.split(df_tr, y_true, groups):\n",
    "        fold = df_tr.iloc[tr_idx]\n",
    "        counts = fold.groupby([\"metric_name\",\"score\"]).size().unstack(fill_value=0)\n",
    "        for c in range(K):\n",
    "            if c not in counts.columns:\n",
    "                counts[c] = 0\n",
    "        counts = counts[sorted(counts.columns)]\n",
    "        n_m = counts.sum(axis=1).astype(float)\n",
    "        global_counts = counts.sum(axis=0).astype(float)\n",
    "        global_prior = (global_counts + alpha) / (global_counts.sum() + alpha*K)\n",
    "\n",
    "        p_emp = (counts + alpha).div((counts + alpha).sum(axis=1), axis=0)\n",
    "        w_m = (n_m / (n_m + lambda_shrink)).clip(0.0, 1.0)\n",
    "        p_m = p_emp.mul(w_m, axis=0).add(global_prior, axis=1).sub(global_prior.mul(w_m, axis=0), axis=0)\n",
    "\n",
    "        for i in va_idx:\n",
    "            m = df_tr.iloc[i][\"metric_name\"]\n",
    "            if m in p_m.index:\n",
    "                oof_P[i] = p_m.loc[m].values.astype(np.float32)\n",
    "            else:\n",
    "                oof_P[i] = global_prior.values.astype(np.float32)\n",
    "    oof_P = np.nan_to_num(oof_P, nan=1.0/K)\n",
    "    oof_P /= np.clip(oof_P.sum(axis=1, keepdims=True), 1e-12, None)\n",
    "    return oof_P\n",
    "\n",
    "oof_P = build_oof_priors(lambda_shrink)\n",
    "\n",
    "\n",
    "best_tau = (1.0, 1.0, 1.0)\n",
    "best_score = None\n",
    "for tau_hi in [1.0, 1.2, 1.5, 2.0, 2.5, 3.0, 4.0]:\n",
    "    for tau_mid in [1.0, 1.1, 1.3, 1.6, 2.0]:\n",
    "        for tau_lo in [1.0, 0.9, 0.8]:\n",
    "            P_t = apply_temperatures(oof_P, tau_lo, tau_mid, tau_hi)\n",
    "            y_pred = P_t.argmax(axis=1)\n",
    "            mae = np.mean(np.abs(y_true - y_pred))\n",
    "            k = qwk(y_true, y_pred)\n",
    "            score = (0.8 * (-mae)) + (0.2 * k)\n",
    "            if (best_score is None) or (score > best_score):\n",
    "                best_score = score\n",
    "                best_tau = (tau_lo, tau_mid, tau_hi)\n",
    "\n",
    "print(\"Chosen temperatures:\", best_tau, \"objective:\", best_score)\n",
    "\n",
    "\n",
    "def apply_tilt(P, eps=0.02):\n",
    "    P = P.copy()\n",
    "    take = np.minimum(P[:,10], eps)\n",
    "    P[:,10] -= take\n",
    "    P[:,9] += take * 0.6\n",
    "    P[:,8] += take * 0.4\n",
    "    P /= np.clip(P.sum(axis=1, keepdims=True), 1e-12, None)\n",
    "    return P\n",
    "\n",
    "P_cal = apply_temperatures(oof_P, *best_tau)\n",
    "if best_tau == (1.0, 1.0, 1.0):\n",
    "    P_cal = apply_tilt(P_cal, eps=0.02)\n",
    "\n",
    "ev_cal = expected_value(P_cal)\n",
    "ent_cal = entropy(P_cal)\n",
    "\n",
    "X_res = np.column_stack([ev_cal, ent_cal, P_cal])\n",
    "X_res = np.nan_to_num(X_res, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "y_res = y_true - np.round(ev_cal)\n",
    "y_res = np.nan_to_num(y_res, nan=0.0)\n",
    "\n",
    "res_model = RidgeCV(alphas=[0.1, 1.0, 5.0, 10.0], fit_intercept=True)\n",
    "res_model.fit(X_res, y_res)\n",
    "\n",
    "# Final priors on full train with same lambda\n",
    "counts_full = df_tr.groupby([\"metric_name\",\"score\"]).size().unstack(fill_value=0)\n",
    "for c in range(K):\n",
    "    if c not in counts_full.columns:\n",
    "        counts_full[c] = 0\n",
    "counts_full = counts_full[sorted(counts_full.columns)]\n",
    "n_m_full = counts_full.sum(axis=1).astype(float)\n",
    "global_counts_full = counts_full.sum(axis=0).astype(float)\n",
    "global_prior_full = (global_counts_full + alpha) / (global_counts_full.sum() + alpha*K)\n",
    "p_emp_full = (counts_full + alpha).div((counts_full + alpha).sum(axis=1), axis=0)\n",
    "w_m_full = (n_m_full / (n_m_full + lambda_shrink)).clip(0.0, 1.0)\n",
    "p_m_full = p_emp_full.mul(w_m_full, axis=0).add(global_prior_full, axis=1).sub(global_prior_full.mul(w_m_full, axis=0), axis=0)\n",
    "\n",
    "metric_prior = {m: p_m_full.loc[m].values.astype(np.float32) for m in p_m_full.index}\n",
    "global_prior_vec = global_prior_full.values.astype(np.float32)\n",
    "\n",
    "def predict_row(r):\n",
    "    # Duplicate fill\n",
    "    h = r[\"combo_hash\"]\n",
    "    if h in dup_cache:\n",
    "        return int(dup_cache[h])\n",
    "    # Metric prior\n",
    "    m = r[\"metric_name\"]\n",
    "    P = metric_prior.get(m, global_prior_vec)\n",
    "    P = P[None, :]\n",
    "    P = apply_temperatures(P, *best_tau)\n",
    "    if best_tau == (1.0, 1.0, 1.0):\n",
    "        P = apply_tilt(P, eps=0.02)\n",
    "    P = np.nan_to_num(P, nan=1.0/K)\n",
    "    P = P / np.clip(P.sum(axis=1, keepdims=True), 1e-12, None)\n",
    "    ev = float((P * idxs[None,:]).sum(axis=1)[0])\n",
    "    ent = float(entropy(P)[0])\n",
    "    x = np.concatenate([[ev, ent], P[0]])\n",
    "    x = np.nan_to_num(x, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    delta = float(res_model.predict(x[None, :])[0])\n",
    "    delta = float(np.clip(delta, -1.0, 0.5))\n",
    "    pred = np.round(ev + delta)\n",
    "    if not np.isfinite(pred):\n",
    "        pred = np.round(ev)  # fallback\n",
    "    pred = int(np.clip(pred, 0, 10))\n",
    "    return pred\n",
    "\n",
    "df_te = df_te.copy()\n",
    "preds = [predict_row(r) for _, r in df_te.iterrows()]\n",
    "submission = pd.DataFrame({\"row_id\": np.arange(len(df_te)), \"score\": preds})\n",
    "print(\"Pred distribution:\", submission[\"score\"].value_counts().sort_index().to_dict())\n",
    "submission.to_csv(\"submission_calibrated_shrunk_residual_safe.csv\", index=False)\n",
    "print(\"Wrote submission_calibrated_shrunk_residual_safe.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f937f73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample columns: ['ID', 'score']\n",
      "Wrote submission_final.csv with columns: ['ID', 'score']\n",
      "   ID  score\n",
      "0   0      9\n",
      "1   1      9\n",
      "2   2      9\n",
      "3   3      9\n",
      "4   4      6\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load  last predictions\n",
    "preds_df = pd.read_csv(\"submission_calibrated_shrunk_residual_safe.csv\")\n",
    "\n",
    "# Read sample to get exact column names and order\n",
    "sample = pd.read_csv(\"sample_submission.csv\")\n",
    "sample_cols = list(sample.columns)\n",
    "print(\"Sample columns:\", sample_cols)\n",
    "\n",
    "# Create a new DataFrame with the same columns\n",
    "out = pd.DataFrame(columns=sample_cols)\n",
    "out[sample_cols[0]] = range(len(preds_df))  # ID column\n",
    "\n",
    "score_col_in_sample = sample_cols[1]\n",
    "if \"score\" in preds_df.columns:\n",
    "    out[score_col_in_sample] = preds_df[\"score\"].astype(int)\n",
    "else:\n",
    "  \n",
    "    pred_col = [c for c in preds_df.columns if c.lower() != sample_cols[0].lower()][0]\n",
    "    out[score_col_in_sample] = preds_df[pred_col].astype(int)\n",
    "\n",
    "# Save with a new name\n",
    "out.to_csv(\"submission_final.csv\", index=False)\n",
    "print(\"Wrote submission_final.csv with columns:\", list(out.columns))\n",
    "print(out.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dc8ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample columns: ['ID', 'score']\n",
      "ID range: 1 to 3638\n",
      "Wrote submission_final.csv with columns: ['ID', 'score'] and 3638 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load  latest predictions \n",
    "preds_df = pd.read_csv(\"submission_calibrated_shrunk_residual_safe.csv\")\n",
    "\n",
    "# Load sample to enforce exact header and order\n",
    "sample = pd.read_csv(\"sample_submission.csv\")\n",
    "sample_cols = list(sample.columns)\n",
    "print(\"Sample columns:\", sample_cols)\n",
    "\n",
    "# Build output with correct columns\n",
    "out = pd.DataFrame(columns=sample_cols)\n",
    "\n",
    "# Set ID from 1..N h)\n",
    "N = len(preds_df)\n",
    "out[sample_cols[0]] = range(1, N + 1)\n",
    "\n",
    "# Map scores to the sample's score column name\n",
    "score_col_in_sample = sample_cols[1]\n",
    "if \"score\" in preds_df.columns:\n",
    "    out[score_col_in_sample] = preds_df[\"score\"].astype(int)\n",
    "else:\n",
    "    \n",
    "    pred_col = [c for c in preds_df.columns if c.lower() != \"id\" and c.lower() != sample_cols[0].lower()][0]\n",
    "    out[score_col_in_sample] = preds_df[pred_col].astype(int)\n",
    "\n",
    "#  sanity checks\n",
    "assert out[sample_cols[0]].iloc[0] == 1, \"ID should start at 1\"\n",
    "assert out[sample_cols[0]].iloc[-1] == N, f\"ID should end at {N}\"\n",
    "print(\"ID range:\", out[sample_cols[0]].min(), \"to\", out[sample_cols[0]].max())\n",
    "\n",
    "# Save final submission\n",
    "out.to_csv(\"submission_final.csv\", index=False)\n",
    "print(\"Wrote submission_final.csv with columns:\", list(out.columns), \"and\", len(out), \"rows\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
